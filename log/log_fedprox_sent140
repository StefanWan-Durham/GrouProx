Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : sent140
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.3
	            model : stacked_lstm
	     model_params : (25, 2, 100)
	               mu : 0.01
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/1.70m flops)
  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform (160.00k/320.00k flops)
    rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform/mul (160.00k/160.00k flops)
    rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform/sub (1/1 flops)
  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform (80.00k/160.00k flops)
    rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform/mul (80.00k/80.00k flops)
    rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/sub (160.00k/160.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/mul_1 (160.00k/160.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/mul (160.00k/160.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/AssignSub (160.00k/160.00k flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/Add (160.00k/160.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/AssignSub (80.00k/80.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/mul (80.00k/80.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/mul_1 (80.00k/80.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/sub (80.00k/80.00k flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/Add (80.00k/80.00k flops)
  dense/kernel/Initializer/random_uniform (3.00k/6.00k flops)
    dense/kernel/Initializer/random_uniform/mul (3.00k/3.00k flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/mul_1 (3.00k/3.00k flops)
  PGD/update_dense/kernel/AssignSub (3.00k/3.00k flops)
  PGD/update_dense/kernel/mul (3.00k/3.00k flops)
  PGD/update_dense/kernel/sub (3.00k/3.00k flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/AssignSub (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/mul (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/mul_1 (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/sub (400/400 flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/Add (400/400 flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/Add (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/AssignSub (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/mul (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/mul_1 (400/400 flops)
  PGD/update_rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/sub (400/400 flops)
  dense_1/kernel/Initializer/random_uniform (30/61 flops)
    dense_1/kernel/Initializer/random_uniform/mul (30/30 flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/bias/mul (30/30 flops)
  PGD/update_dense_1/kernel/mul_1 (30/30 flops)
  PGD/update_dense_1/kernel/mul (30/30 flops)
  PGD/update_dense_1/kernel/AssignSub (30/30 flops)
  PGD/update_dense_1/kernel/sub (30/30 flops)
  PGD/update_dense/bias/sub (30/30 flops)
  PGD/update_dense/bias/AssignSub (30/30 flops)
  PGD/update_dense/bias/mul_1 (30/30 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_scalar (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/Equal (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_scalar (1/1 flops)
  rnn/while/Less_1 (1/1 flops)
  rnn/while/Less (1/1 flops)
  rnn/Minimum (1/1 flops)
  rnn/Maximum (1/1 flops)
  gradients/sigmoid_cross_entropy_loss/value_grad/mul (1/1 flops)
  gradients/sigmoid_cross_entropy_loss/value_grad/Neg (1/1 flops)
  PGD/update_dense_1/bias/AssignSub (1/1 flops)
  PGD/update_dense_1/bias/mul (1/1 flops)
  PGD/update_dense_1/bias/mul_1 (1/1 flops)
  gradients/Sub (1/1 flops)
  gradients/GreaterEqual (1/1 flops)
  gradients/Add (1/1 flops)
  PGD/update_dense_1/bias/sub (1/1 flops)

======================End of Report==========================
772 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.41324846770391327
At round 0 training accuracy: 0.4167311681476207
At round 0 training loss: 0.6979609710881797
gradient difference: 1.3072913666055992
At round 1 accuracy: 0.6216407355021216
At round 1 training accuracy: 0.620762252701322
At round 1 training loss: 0.6604853315260607
gradient difference: 1.1210144244032092
At round 2 accuracy: 0.6032531824611033
At round 2 training accuracy: 0.5963342518344221
At round 2 training loss: 0.6770692241295576
gradient difference: 1.4976254330877743
At round 3 accuracy: 0.5604667609618105
At round 3 training accuracy: 0.5618130592278399
At round 3 training loss: 0.7083898766043053
gradient difference: 1.5140819872336673
At round 4 accuracy: 0.5706034889203206
At round 4 training accuracy: 0.5730208365584074
At round 4 training loss: 0.859448825838243
gradient difference: 9.334928486878935
At round 5 accuracy: 0.5332390381895332
At round 5 training accuracy: 0.5355274157094647
At round 5 training loss: 1.0267886590634603
gradient difference: 23.243791691360222
At round 6 accuracy: 0.4681753889674682
At round 6 training accuracy: 0.4704170407752562
At round 6 training loss: 1.1591921006087869
gradient difference: 19.303663797683896
At round 7 accuracy: 0.6198727015558698
At round 7 training accuracy: 0.6206074491470324
At round 7 training loss: 1.5154758791589953
gradient difference: 6.9376596916988476
At round 8 accuracy: 0.5834512022630834
At round 8 training accuracy: 0.5819375212854887
At round 8 training loss: 1.4981565413475222
gradient difference: 39.22994163366957
At round 9 accuracy: 0.5736680810938237
At round 9 training accuracy: 0.5757453791139044
At round 9 training loss: 0.9833956224245661
gradient difference: 16.463575870830514
At round 10 accuracy: 0.5121404997642621
At round 10 training accuracy: 0.5179726926530233
At round 10 training loss: 1.382869470112187
gradient difference: 45.38776669983639
At round 11 accuracy: 0.5008250825082509
At round 11 training accuracy: 0.5105111613362643
At round 11 training loss: 1.0254329936304654
gradient difference: 3.3303363275217093
At round 12 accuracy: 0.5960631777463461
At round 12 training accuracy: 0.5981918944858974
At round 12 training loss: 1.004143126851524
gradient difference: 6.383849142630627
At round 13 accuracy: 0.518033946251768
At round 13 training accuracy: 0.51846806402675
At round 13 training loss: 0.9982245797634239
gradient difference: 9.703571236474096
At round 14 accuracy: 0.5236916548797736
At round 14 training accuracy: 0.5195207281959193
At round 14 training loss: 1.0398575811900015
gradient difference: 19.43950181641454
At round 15 accuracy: 0.5794436586515794
At round 15 training accuracy: 0.5729279544258337
At round 15 training loss: 1.3475210324456288
gradient difference: 33.22503102200409
At round 16 accuracy: 0.6073785950023574
At round 16 training accuracy: 0.6085946933341589
At round 16 training loss: 1.3856622853184282
gradient difference: 13.877554006689529
At round 17 accuracy: 0.5988920320603489
At round 17 training accuracy: 0.6002662621133781
At round 17 training loss: 1.2725514041895931
gradient difference: 26.27154326204904
At round 18 accuracy: 0.562942008486563
At round 18 training accuracy: 0.56568314808508
At round 18 training loss: 1.1471551236365205
gradient difference: 16.298246263713178
At round 19 accuracy: 0.6065535124941066
At round 19 training accuracy: 0.6185021208086938
At round 19 training loss: 1.0227195672881637
gradient difference: 12.311908756241555
At round 20 accuracy: 0.563059877416313
At round 20 training accuracy: 0.5768290039939317
At round 20 training loss: 1.2201273621973803
gradient difference: 31.23548433557924
At round 21 accuracy: 0.6165723715228666
At round 21 training accuracy: 0.6205455277253166
At round 21 training loss: 1.6422573818098112
gradient difference: 9.676238248235622
At round 22 accuracy: 0.5516265912305516
At round 22 training accuracy: 0.5622155484689928
At round 22 training loss: 1.3769979478108092
gradient difference: 31.737027552174975
At round 23 accuracy: 0.6272984441301273
At round 23 training accuracy: 0.624322734449983
At round 23 training loss: 1.0221843531313544
gradient difference: 7.350342608932719
At round 24 accuracy: 0.6067892503536068
At round 24 training accuracy: 0.6077277934301372
At round 24 training loss: 1.1740955087376852
gradient difference: 17.39545124676905
At round 25 accuracy: 0.6037246581801037
At round 25 training accuracy: 0.611721725130809
At round 25 training loss: 1.0359271618904962
gradient difference: 12.101198920135548
At round 26 accuracy: 0.6089108910891089
At round 26 training accuracy: 0.6052199758506456
At round 26 training loss: 1.1192107573129297
gradient difference: 11.585567126745282
At round 27 accuracy: 0.6090287600188591
At round 27 training accuracy: 0.6104213752747764
At round 27 training loss: 1.1697295268216004
gradient difference: 13.819310240273278
At round 28 accuracy: 0.5638849599245639
At round 28 training accuracy: 0.5729279544258337
At round 28 training loss: 1.19023513368383
gradient difference: 18.615104027620227
At round 29 accuracy: 0.6124469589816125
At round 29 training accuracy: 0.6151274033251803
At round 29 training loss: 1.122035016989544
gradient difference: 9.197127701475944
At round 30 accuracy: 0.6103253182461104
At round 30 training accuracy: 0.6112882751787981
At round 30 training loss: 1.3046537428100338
gradient difference: 20.154578886808924
At round 31 accuracy: 0.5821546440358322
At round 31 training accuracy: 0.5847859066844174
At round 31 training loss: 1.1168903035140625
gradient difference: 18.58111035704506
At round 32 accuracy: 0.5896982555398397
At round 32 training accuracy: 0.5916901452057339
At round 32 training loss: 1.2753112515123959
gradient difference: 25.24131569591226
At round 33 accuracy: 0.6139792550683639
At round 33 training accuracy: 0.6202668813275953
At round 33 training loss: 1.0243838761263382
gradient difference: 7.126541409953512
At round 34 accuracy: 0.5453795379537953
At round 34 training accuracy: 0.5564258955385616
At round 34 training loss: 1.3753696085206093
gradient difference: 18.665658274129736
At round 35 accuracy: 0.5921735030645922
At round 35 training accuracy: 0.6008235549088207
At round 35 training loss: 1.223144426148783
gradient difference: 20.07543321995258
At round 36 accuracy: 0.6096181046676096
At round 36 training accuracy: 0.6177281030372458
At round 36 training loss: 1.0650412290880298
gradient difference: 17.127872973446724
At round 37 accuracy: 0.5937057991513437
At round 37 training accuracy: 0.6043530759466237
At round 37 training loss: 0.951341744947247
gradient difference: 11.831676081691924
At round 38 accuracy: 0.6224658180103725
At round 38 training accuracy: 0.6323725192730425
At round 38 training loss: 0.8635253643307222
gradient difference: 9.478840324438977
At round 39 accuracy: 0.6206977840641207
At round 39 training accuracy: 0.6351899439611134
At round 39 training loss: 0.9592487209467228
gradient difference: 11.582604611255068
At round 40 accuracy: 0.623998114097124
At round 40 training accuracy: 0.6296479767175455
At round 40 training loss: 1.0378397561032964
gradient difference: 14.837371719692383
At round 41 accuracy: 0.6000707213578501
At round 41 training accuracy: 0.6037957831511811
At round 41 training loss: 0.9759817002541983
gradient difference: 11.129346395152163
At round 42 accuracy: 0.6036067892503536
At round 42 training accuracy: 0.600328183535094
At round 42 training loss: 1.1688952514827848
gradient difference: 19.06037936097447
At round 43 accuracy: 0.628948609146629
At round 43 training accuracy: 0.6360258831542772
At round 43 training loss: 0.7844126025369877
gradient difference: 6.530383109693939
At round 44 accuracy: 0.6387317303158887
At round 44 training accuracy: 0.6507631815226478
At round 44 training loss: 0.8154439568087753
gradient difference: 8.374977215871713
At round 45 accuracy: 0.5931164545025931
At round 45 training accuracy: 0.6008545156196786
At round 45 training loss: 0.9261361360961592
gradient difference: 12.145745745874335
At round 46 accuracy: 0.621051390853371
At round 46 training accuracy: 0.6247561844019939
At round 46 training loss: 0.9969532777549248
gradient difference: 14.856566140168942
At round 47 accuracy: 0.639085337105139
At round 47 training accuracy: 0.6487816960277408
At round 47 training loss: 0.8196127299905621
gradient difference: 6.628809641934898
At round 48 accuracy: 0.642975011786893
At round 48 training accuracy: 0.6505154958357844
At round 48 training loss: 0.9995836503021358
gradient difference: 11.604797881965073
At round 49 accuracy: 0.6176331918906176
At round 49 training accuracy: 0.6211337812316171
At round 49 training loss: 0.8949397441836002
gradient difference: 11.157991082220573
At round 50 accuracy: 0.6251768033946252
At round 50 training accuracy: 0.6282547447289389
At round 50 training loss: 0.8888138972742459
gradient difference: 6.568306764385241
At round 51 accuracy: 0.6165723715228666
At round 51 training accuracy: 0.6200811170624477
At round 51 training loss: 1.1759787285281404
gradient difference: 23.370280845733483
At round 52 accuracy: 0.6360207449316361
At round 52 training accuracy: 0.6408867147589709
At round 52 training loss: 1.0080121121473367
gradient difference: 11.453499120710742
At round 53 accuracy: 0.6362564827911362
At round 53 training accuracy: 0.636118765286851
At round 53 training loss: 1.2163029286083462
gradient difference: 13.054594989900643
At round 54 accuracy: 0.6486327204148986
At round 54 training accuracy: 0.6456546642310907
At round 54 training loss: 0.9390610399033579
gradient difference: 9.269627160966253
At round 55 accuracy: 0.6478076379066479
At round 55 training accuracy: 0.6466763676894022
At round 55 training loss: 1.0189701577498813
gradient difference: 13.431129793866292
At round 56 accuracy: 0.637906647807638
At round 56 training accuracy: 0.6403294219635283
At round 56 training loss: 0.851318295072455
gradient difference: 8.233069890054958
At round 57 accuracy: 0.6375530410183875
At round 57 training accuracy: 0.6331155763336326
At round 57 training loss: 0.9610994205566955
gradient difference: 14.860212222757808
At round 58 accuracy: 0.6268269684111268
At round 58 training accuracy: 0.6311650515495836
At round 58 training loss: 1.137655675486811
gradient difference: 20.447013574854168
At round 59 accuracy: 0.6603017444601603
At round 59 training accuracy: 0.663271308709248
At round 59 training loss: 0.9597521570386223
gradient difference: 7.774232820130439
At round 60 accuracy: 0.6244695898161244
At round 60 training accuracy: 0.6260874949688845
At round 60 training loss: 0.9114761835690149
gradient difference: 9.529689817094729
At round 61 accuracy: 0.6522866572371523
At round 61 training accuracy: 0.6551596024644726
At round 61 training loss: 1.0786225111170593
gradient difference: 20.04946269433039
At round 62 accuracy: 0.657001414427157
At round 62 training accuracy: 0.6638286015046906
At round 62 training loss: 0.9452199184434324
gradient difference: 13.935128604314484
At round 63 accuracy: 0.5902876001885903
At round 63 training accuracy: 0.5982228551967553
At round 63 training loss: 0.9974500984023688
gradient difference: 12.761509860789632
At round 64 accuracy: 0.6397925506836398
At round 64 training accuracy: 0.6474503854608502
At round 64 training loss: 1.0672798925066564
gradient difference: 12.235308310833704
At round 65 accuracy: 0.6351956624233852
At round 65 training accuracy: 0.6389981113966376
At round 65 training loss: 0.8440000749263603
gradient difference: 7.738326643709571
At round 66 accuracy: 0.6518151815181518
At round 66 training accuracy: 0.6530852348369919
At round 66 training loss: 0.9808238128246809
gradient difference: 12.534032916722762
At round 67 accuracy: 0.6565299387081566
At round 67 training accuracy: 0.6618780767206415
At round 67 training loss: 1.004831522873529
gradient difference: 10.893154506769589
At round 68 accuracy: 0.6553512494106554
At round 68 training accuracy: 0.6598346698040187
At round 68 training loss: 0.9253057848182932
gradient difference: 12.109808797028016
At round 69 accuracy: 0.6598302687411598
At round 69 training accuracy: 0.6599585126474504
At round 69 training loss: 0.9628531120185635
gradient difference: 11.154076108033008
At round 70 accuracy: 0.5766148043375766
At round 70 training accuracy: 0.5871389207096195
At round 70 training loss: 1.2189466580392454
gradient difference: 15.569106322423387
At round 71 accuracy: 0.6324846770391325
At round 71 training accuracy: 0.6359949224434193
At round 71 training loss: 1.0521126658546969
gradient difference: 18.85683304442649
At round 72 accuracy: 0.6598302687411598
At round 72 training accuracy: 0.6645716585652807
At round 72 training loss: 1.0008289526185077
gradient difference: 11.606378064682179
At round 73 accuracy: 0.652050919377652
At round 73 training accuracy: 0.660918294684046
At round 73 training loss: 1.0149620335518466
gradient difference: 13.468079429161671
At round 74 accuracy: 0.661008958038661
At round 74 training accuracy: 0.6696492151459797
At round 74 training loss: 1.0244600824499912
gradient difference: 12.524507743579091
At round 75 accuracy: 0.639085337105139
At round 75 training accuracy: 0.6479457568345769
At round 75 training loss: 0.9153936991329364
gradient difference: 10.519166764715694
At round 76 accuracy: 0.6107967939651108
At round 76 training accuracy: 0.6098331217684758
At round 76 training loss: 1.0313469704905252
gradient difference: 14.971820324118667
At round 77 accuracy: 0.6538189533239038
At round 77 training accuracy: 0.6629926623115266
At round 77 training loss: 0.9194056943143726
gradient difference: 9.968102446696442
At round 78 accuracy: 0.6575907590759076
At round 78 training accuracy: 0.6687203938202421
At round 78 training loss: 1.0269658492851015
gradient difference: 12.222434723815573
At round 79 accuracy: 0.603017444601603
At round 79 training accuracy: 0.6156846961206229
At round 79 training loss: 1.1024307009430028
gradient difference: 14.949945417570431
At round 80 accuracy: 0.6478076379066479
At round 80 training accuracy: 0.6537044490541503
At round 80 training loss: 1.1032328278011585
gradient difference: 10.041069807513228
At round 81 accuracy: 0.6433286185761433
At round 81 training accuracy: 0.6481315210997245
At round 81 training loss: 0.9229423386302392
gradient difference: 9.239706513448679
At round 82 accuracy: 0.6493399339933993
At round 82 training accuracy: 0.6576674200439642
At round 82 training loss: 0.9169942638191091
gradient difference: 10.18394095295937
At round 83 accuracy: 0.6032531824611033
At round 83 training accuracy: 0.6178209851698195
At round 83 training loss: 0.9585726951110711
gradient difference: 13.306982414961258
At round 84 accuracy: 0.5927628477133428
At round 84 training accuracy: 0.6090900647078857
At round 84 training loss: 1.0440914173120872
gradient difference: 15.550376474787557
At round 85 accuracy: 0.6395568128241396
At round 85 training accuracy: 0.6470478962196972
At round 85 training loss: 1.2852180913353812
gradient difference: 11.43547484283953
At round 86 accuracy: 0.6448609146628949
At round 86 training accuracy: 0.6528994705718443
At round 86 training loss: 1.2492992688264757
gradient difference: 15.752727482895683
At round 87 accuracy: 0.6343705799151343
At round 87 training accuracy: 0.6582866342611227
At round 87 training loss: 1.125267857064628
gradient difference: 14.257540012666247
At round 88 accuracy: 0.6263554926921263
At round 88 training accuracy: 0.6494318709557572
At round 88 training loss: 1.0492338585408407
gradient difference: 12.587336392977194
At round 89 accuracy: 0.641914191419142
At round 89 training accuracy: 0.6593392984302919
At round 89 training loss: 0.9447598862498402
gradient difference: 10.535531443018264
At round 90 accuracy: 0.6615983026874116
At round 90 training accuracy: 0.6729000897860615
At round 90 training loss: 0.9810658075274661
gradient difference: 11.044359414494775
At round 91 accuracy: 0.6636020744931636
At round 91 training accuracy: 0.6709805257128704
At round 91 training loss: 1.2330426664076584
gradient difference: 13.707835543026738
At round 92 accuracy: 0.6615983026874116
At round 92 training accuracy: 0.671537818508313
At round 92 training loss: 1.086079252796748
gradient difference: 11.603193426465365
At round 93 accuracy: 0.6565299387081566
At round 93 training accuracy: 0.6664602619276139
At round 93 training loss: 1.2506828155396421
gradient difference: 10.798361401220488
At round 94 accuracy: 0.656058462989156
At round 94 training accuracy: 0.6697111365676955
At round 94 training loss: 1.0812301374824054
gradient difference: 15.323624489685216
At round 95 accuracy: 0.6511079679396511
At round 95 training accuracy: 0.6631784265766743
At round 95 training loss: 0.951861363965539
gradient difference: 13.379944909246973
At round 96 accuracy: 0.6685525695426685
At round 96 training accuracy: 0.6769559429084492
At round 96 training loss: 0.814133159444878
gradient difference: 5.4871356989231375
At round 97 accuracy: 0.6264733616218765
At round 97 training accuracy: 0.6378216043840367
At round 97 training loss: 0.9858329800347732
gradient difference: 16.49734924172741
At round 98 accuracy: 0.6582979726544083
At round 98 training accuracy: 0.6749434967026843
At round 98 training loss: 1.0338769175251468
gradient difference: 14.86661913118961
At round 99 accuracy: 0.6430928807166431
At round 99 training accuracy: 0.6581008699959751
At round 99 training loss: 0.955202797412138
gradient difference: 13.501408938365135
At round 100 accuracy: 0.6308345120226309
At round 100 training accuracy: 0.6408247933372551
At round 100 training loss: 1.114241282396628
gradient difference: 24.625820831511287
At round 101 accuracy: 0.6617161716171617
At round 101 training accuracy: 0.675067339546116
At round 101 training loss: 1.0346310573895388
gradient difference: 13.043161437422139
At round 102 accuracy: 0.6603017444601603
At round 102 training accuracy: 0.6721570327254713
At round 102 training loss: 1.1884709635992772
gradient difference: 19.348872881777123
At round 103 accuracy: 0.6711456859971712
At round 103 training accuracy: 0.6855630205269513
At round 103 training loss: 1.1215669522413363
gradient difference: 15.288042047623478
At round 104 accuracy: 0.6542904290429042
At round 104 training accuracy: 0.6712282113997338
At round 104 training loss: 0.9347398419233334
gradient difference: 11.264577598604038
At round 105 accuracy: 0.6360207449316361
At round 105 training accuracy: 0.6584723985262702
At round 105 training loss: 0.921253469170971
gradient difference: 12.709985899649489
At round 106 accuracy: 0.6347241867043847
At round 106 training accuracy: 0.6454379392550853
At round 106 training loss: 0.7963365187589849
gradient difference: 6.944274823868444
At round 107 accuracy: 0.6573550212164073
At round 107 training accuracy: 0.670361311495712
At round 107 training loss: 0.7766829186738993
gradient difference: 7.048813183394974
At round 108 accuracy: 0.6544082979726544
At round 108 training accuracy: 0.6736121861357937
At round 108 training loss: 0.9562363529227563
gradient difference: 15.130250286242939
At round 109 accuracy: 0.6568835454974069
At round 109 training accuracy: 0.6707018793151491
At round 109 training loss: 1.0495320842915141
gradient difference: 17.085657944992526
At round 110 accuracy: 0.6694955209806694
At round 110 training accuracy: 0.6781943713427661
At round 110 training loss: 1.0162150247267263
gradient difference: 11.157453875666947
At round 111 accuracy: 0.6441537010843942
At round 111 training accuracy: 0.6605777268646088
At round 111 training loss: 0.9328959362561678
gradient difference: 11.25874931891965
At round 112 accuracy: 0.6586515794436586
At round 112 training accuracy: 0.6748506145701105
At round 112 training loss: 0.8326980994720761
gradient difference: 9.156500407551478
At round 113 accuracy: 0.6527581329561527
At round 113 training accuracy: 0.6699278615437011
At round 113 training loss: 0.8658100264902385
gradient difference: 10.73011122716065
At round 114 accuracy: 0.6298915605846299
At round 114 training accuracy: 0.6479767175454348
At round 114 training loss: 1.0487241342431843
gradient difference: 20.382535145358922
At round 115 accuracy: 0.6768033946251768
At round 115 training accuracy: 0.6908573020836558
At round 115 training loss: 1.0102829234290822
gradient difference: 11.355600259993558
At round 116 accuracy: 0.6687883074021688
At round 116 training accuracy: 0.6828694386823121
At round 116 training loss: 1.1427968128966188
gradient difference: 15.995876250579204
At round 117 accuracy: 0.6684347006129184
At round 117 training accuracy: 0.6795876033313725
At round 117 training loss: 1.189047699524321
gradient difference: 15.738943678100924
At round 118 accuracy: 0.6725601131541725
At round 118 training accuracy: 0.688442366636738
At round 118 training loss: 0.8784046175326982
gradient difference: 10.40072042607233
At round 119 accuracy: 0.6738566713814239
At round 119 training accuracy: 0.6899594414687761
At round 119 training loss: 0.9652752527075888
gradient difference: 12.439434980343458
At round 120 accuracy: 0.6162187647336163
At round 120 training accuracy: 0.6280999411746494
At round 120 training loss: 1.0284171270972355
gradient difference: 11.174907538797545
At round 121 accuracy: 0.6512258368694013
At round 121 training accuracy: 0.6689680795071055
At round 121 training loss: 0.8854045371316863
gradient difference: 11.633836059093648
At round 122 accuracy: 0.6283592644978784
At round 122 training accuracy: 0.6517848849809592
At round 122 training loss: 0.9314609861587634
gradient difference: 10.664396589624152
At round 123 accuracy: 0.6252946723243753
At round 123 training accuracy: 0.6380073686491842
At round 123 training loss: 0.9567489180731885
gradient difference: 10.429597567041233
At round 124 accuracy: 0.6581801037246582
At round 124 training accuracy: 0.6761200037152854
At round 124 training loss: 0.8433570067689319
gradient difference: 9.26134834312536
At round 125 accuracy: 0.6706742102781706
At round 125 training accuracy: 0.6818786959348586
At round 125 training loss: 1.2364651139682032
gradient difference: 18.675374242248658
At round 126 accuracy: 0.6707920792079208
At round 126 training accuracy: 0.6894950308059073
At round 126 training loss: 0.9798829292335733
gradient difference: 15.05144144035098
At round 127 accuracy: 0.6704384724186704
At round 127 training accuracy: 0.6869872132264156
At round 127 training loss: 0.9858369304275495
gradient difference: 12.863610086722451
At round 128 accuracy: 0.6475719000471476
At round 128 training accuracy: 0.6633332301309638
At round 128 training loss: 0.9979585430637269
gradient difference: 15.42334838788608
At round 129 accuracy: 0.6558227251296558
At round 129 training accuracy: 0.6759342394501378
At round 129 training loss: 1.0057300244052363
gradient difference: 15.74177484017417
At round 130 accuracy: 0.6648986327204149
At round 130 training accuracy: 0.6833648100560389
At round 130 training loss: 1.0028899311500608
gradient difference: 15.119382753540764
At round 131 accuracy: 0.6401461574728902
At round 131 training accuracy: 0.6598037090931608
At round 131 training loss: 0.9795066919900649
gradient difference: 14.137429590960394
At round 132 accuracy: 0.6330740216878831
At round 132 training accuracy: 0.6516300814266696
At round 132 training loss: 0.9851173139870586
gradient difference: 11.033039041361395
At round 133 accuracy: 0.6803394625176803
At round 133 training accuracy: 0.6929626304219945
At round 133 training loss: 1.011508277582899
gradient difference: 10.762550666937395
At round 134 accuracy: 0.6574728901461575
At round 134 training accuracy: 0.6787826248490665
At round 134 training loss: 0.9436581981652785
gradient difference: 13.875661871136343
At round 135 accuracy: 0.6235266383781235
At round 135 training accuracy: 0.6459023499179541
At round 135 training loss: 1.0466551247657996
gradient difference: 18.333034423355382
At round 136 accuracy: 0.669024045261669
At round 136 training accuracy: 0.6830242422366017
At round 136 training loss: 0.9940287188659178
gradient difference: 15.135546332806683
At round 137 accuracy: 0.6834040546911834
At round 137 training accuracy: 0.6981949905569832
At round 137 training loss: 0.9304422529334944
gradient difference: 11.316264227716575
At round 138 accuracy: 0.6865865157944366
At round 138 training accuracy: 0.7029010186073872
At round 138 training loss: 0.9451142686656085
gradient difference: 11.30948265101282
At round 139 accuracy: 0.6780999528524281
At round 139 training accuracy: 0.6896807950710548
At round 139 training loss: 0.8982964308419085
gradient difference: 11.62581073155053
At round 140 accuracy: 0.6818717586044318
At round 140 training accuracy: 0.6909501842162296
At round 140 training loss: 0.9892752747929902
gradient difference: 10.228959615681234
At round 141 accuracy: 0.6892975011786893
At round 141 training accuracy: 0.7038298399331249
At round 141 training loss: 0.8957246785904602
gradient difference: 9.917165006143582
At round 142 accuracy: 0.6714992927864215
At round 142 training accuracy: 0.6826217529954488
At round 142 training loss: 1.3158026674304288
gradient difference: 14.16502324800411
At round 143 accuracy: 0.6623055162659123
At round 143 training accuracy: 0.6775441964147497
At round 143 training loss: 0.9580358963669299
gradient difference: 13.834047858520101
At round 144 accuracy: 0.6724422442244224
At round 144 training accuracy: 0.6874516238892845
At round 144 training loss: 0.8402610095595267
gradient difference: 10.306294145191265
At round 145 accuracy: 0.6857614332861858
At round 145 training accuracy: 0.7017245115947862
At round 145 training loss: 0.8726763524487164
gradient difference: 8.547226244703106
At round 146 accuracy: 0.6823432343234324
At round 146 training accuracy: 0.7000216724976005
At round 146 training loss: 0.8941853534878107
gradient difference: 10.598465230539679
At round 147 accuracy: 0.6247053276756247
At round 147 training accuracy: 0.6414130468435555
At round 147 training loss: 0.6592328207004313
gradient difference: 2.2278107914412115
At round 148 accuracy: 0.6211692597831212
At round 148 training accuracy: 0.6293074088981083
At round 148 training loss: 0.9266016180043872
gradient difference: 10.360669886266825
At round 149 accuracy: 0.6465110796793965
At round 149 training accuracy: 0.6606396482863246
At round 149 training loss: 1.1446173411611253
gradient difference: 16.324883613386735
At round 150 accuracy: 0.6737388024516737
At round 150 training accuracy: 0.6866156846961207
At round 150 training loss: 0.8873885477479972
gradient difference: 10.698263325553246
At round 151 accuracy: 0.6796322489391796
At round 151 training accuracy: 0.689216384408186
At round 151 training loss: 1.0217623852534667
gradient difference: 12.940846151911712
At round 152 accuracy: 0.654997642621405
At round 152 training accuracy: 0.6642001300349856
At round 152 training loss: 0.9359881296757805
gradient difference: 12.514864018247453
At round 153 accuracy: 0.6614804337576615
At round 153 training accuracy: 0.6811356388742685
At round 153 training loss: 0.874935574617559
gradient difference: 11.735880383771336
At round 154 accuracy: 0.6714992927864215
At round 154 training accuracy: 0.682126381621722
At round 154 training loss: 0.9189864169847359
gradient difference: 12.547388699586664
At round 155 accuracy: 0.6718528995756718
At round 155 training accuracy: 0.6799281711508096
At round 155 training loss: 0.8964157451366235
gradient difference: 8.312861154293376
At round 156 accuracy: 0.6653701084394154
At round 156 training accuracy: 0.6759961608718537
At round 156 training loss: 1.0344254540561884
gradient difference: 14.233985665677652
At round 157 accuracy: 0.6805752003771806
At round 157 training accuracy: 0.6939843338803059
At round 157 training loss: 0.9531641484805827
gradient difference: 12.072307135167632
At round 158 accuracy: 0.6793965110796794
At round 158 training accuracy: 0.6904857735533608
At round 158 training loss: 0.9270981207273479
gradient difference: 12.249126561513354
At round 159 accuracy: 0.651933050447902
At round 159 training accuracy: 0.6603610018886034
At round 159 training loss: 0.9782406197892094
gradient difference: 16.605666769297642
At round 160 accuracy: 0.6896511079679396
At round 160 training accuracy: 0.7046967398371466
At round 160 training loss: 0.9760077968722742
gradient difference: 14.879282925369056
At round 161 accuracy: 0.683050447901933
At round 161 training accuracy: 0.6973900120746772
At round 161 training loss: 0.9356878718571447
gradient difference: 12.734740275916705
At round 162 accuracy: 0.669024045261669
At round 162 training accuracy: 0.6830552029474597
At round 162 training loss: 0.7882509902921809
gradient difference: 7.522677761928775
At round 163 accuracy: 0.6882366808109383
At round 163 training accuracy: 0.6999597510758847
At round 163 training loss: 0.8815975225750543
gradient difference: 9.344863339399883
At round 164 accuracy: 0.6802215935879302
At round 164 training accuracy: 0.694727390940896
At round 164 training loss: 0.9749354744631451
gradient difference: 12.213679419875426
At round 165 accuracy: 0.6805752003771806
At round 165 training accuracy: 0.6976376977615406
At round 165 training loss: 0.9338401937878139
gradient difference: 13.157508609854796
At round 166 accuracy: 0.6872937293729373
At round 166 training accuracy: 0.7013529830644911
At round 166 training loss: 0.891673774554674
gradient difference: 11.344812388504124
At round 167 accuracy: 0.6387317303158887
At round 167 training accuracy: 0.6582866342611227
At round 167 training loss: 1.0471324047430317
gradient difference: 14.202084422830396
At round 168 accuracy: 0.65994813767091
At round 168 training accuracy: 0.6826217529954488
At round 168 training loss: 0.888116112744024
gradient difference: 10.269684665483666
At round 169 accuracy: 0.6785714285714286
At round 169 training accuracy: 0.7001764760518902
At round 169 training loss: 0.8540869226714153
gradient difference: 8.731843208387545
At round 170 accuracy: 0.6691419141914191
At round 170 training accuracy: 0.695129880182049
At round 170 training loss: 0.8872433243178435
gradient difference: 11.388739235610482
At round 171 accuracy: 0.69000471475719
At round 171 training accuracy: 0.7077618502120808
At round 171 training loss: 0.905491886006085
gradient difference: 11.645270135549456
At round 172 accuracy: 0.6809288071664309
At round 172 training accuracy: 0.7007337688473327
At round 172 training loss: 0.7942048112336579
gradient difference: 8.252265735095378
At round 173 accuracy: 0.6857614332861858
At round 173 training accuracy: 0.7020341187033654
At round 173 training loss: 0.9159248500886498
gradient difference: 10.511387965470476
At round 174 accuracy: 0.6757425742574258
At round 174 training accuracy: 0.6952227623146228
At round 174 training loss: 0.8643143774745403
gradient difference: 10.893560160645256
At round 175 accuracy: 0.6712635549269212
At round 175 training accuracy: 0.695903897953497
At round 175 training loss: 0.9084798993127344
gradient difference: 10.788465999748215
At round 176 accuracy: 0.6731494578029231
At round 176 training accuracy: 0.6910430663488034
At round 176 training loss: 0.9716386694382699
gradient difference: 10.315749525768304
At round 177 accuracy: 0.6762140499764262
At round 177 training accuracy: 0.7016625901730704
At round 177 training loss: 0.8388588906141461
gradient difference: 9.522618214736896
At round 178 accuracy: 0.6595945308816596
At round 178 training accuracy: 0.6856249419486672
At round 178 training loss: 0.9026357678632011
gradient difference: 10.629274154078098
At round 179 accuracy: 0.6795143800094295
At round 179 training accuracy: 0.7035202328245457
At round 179 training loss: 0.8881247368817095
gradient difference: 9.465797180206607
At round 180 accuracy: 0.6867043847241867
At round 180 training accuracy: 0.712436917551627
At round 180 training loss: 0.8441886749450623
gradient difference: 9.376581059363996
At round 181 accuracy: 0.6752710985384253
At round 181 training accuracy: 0.704170407752562
At round 181 training loss: 0.8553979954120134
gradient difference: 10.475699526361737
At round 182 accuracy: 0.6849363507779349
At round 182 training accuracy: 0.7099910213938512
At round 182 training loss: 0.8366178732857198
gradient difference: 8.44128734134006
At round 183 accuracy: 0.6732673267326733
At round 183 training accuracy: 0.7065543824886219
At round 183 training loss: 0.8146379325255172
gradient difference: 9.226970873702344
At round 184 accuracy: 0.6782178217821783
At round 184 training accuracy: 0.7065543824886219
At round 184 training loss: 0.8274461951363743
gradient difference: 8.673848588230355
At round 185 accuracy: 0.6733851956624234
At round 185 training accuracy: 0.6956252515557757
At round 185 training loss: 1.1122130369924537
gradient difference: 13.751835395058901
At round 186 accuracy: 0.688000942951438
At round 186 training accuracy: 0.7098981392612774
At round 186 training loss: 0.92073943393611
gradient difference: 10.276080067302722
At round 187 accuracy: 0.6885902876001886
At round 187 training accuracy: 0.7098052571287037
At round 187 training loss: 0.8846349088449542
gradient difference: 9.946202587094474
At round 188 accuracy: 0.6845827439886846
At round 188 training accuracy: 0.7043871327285675
At round 188 training loss: 0.909490847667784
gradient difference: 9.138034702297249
At round 189 accuracy: 0.6791607732201792
At round 189 training accuracy: 0.700238397473606
At round 189 training loss: 0.8668778166325909
gradient difference: 8.595180441124102
At round 190 accuracy: 0.6722065063649222
At round 190 training accuracy: 0.6923124554939781
At round 190 training loss: 0.8486695015816704
gradient difference: 8.800722792795927
At round 191 accuracy: 0.6449787835926449
At round 191 training accuracy: 0.6694634508808323
At round 191 training loss: 0.877183487714954
gradient difference: 10.763544718820102
At round 192 accuracy: 0.675035360678925
At round 192 training accuracy: 0.6991238118827209
At round 192 training loss: 0.7846728265685498
gradient difference: 7.917894461772276
At round 193 accuracy: 0.6845827439886846
At round 193 training accuracy: 0.7083501037183814
At round 193 training loss: 0.8461996417215913
gradient difference: 8.274738138402364
At round 194 accuracy: 0.6855256954266855
At round 194 training accuracy: 0.7012291402210594
At round 194 training loss: 1.0496237864463371
gradient difference: 10.63207326903444
At round 195 accuracy: 0.6852899575671852
At round 195 training accuracy: 0.7099600606829933
At round 195 training loss: 0.8584985906235295
gradient difference: 9.388448333477966
At round 196 accuracy: 0.6818717586044318
At round 196 training accuracy: 0.7074832038143596
At round 196 training loss: 0.8318845386894937
gradient difference: 9.633640726437374
At round 197 accuracy: 0.6842291371994342
At round 197 training accuracy: 0.7064305396451902
At round 197 training loss: 1.0473440685355881
gradient difference: 14.856500759435237
At round 198 accuracy: 0.6740924092409241
At round 198 training accuracy: 0.6993405368587263
At round 198 training loss: 0.9913474048662894
gradient difference: 17.535021823869332
At round 199 accuracy: 0.6663130598774163
At round 199 training accuracy: 0.6866156846961207
At round 199 training loss: 0.8677681966234524
gradient difference: 7.4483914858887275
At round 200 accuracy: 0.6683168316831684
At round 200 training accuracy: 0.6879469952630113
